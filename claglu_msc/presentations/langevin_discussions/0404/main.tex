
% Template by Arnau, based on:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[.08pt,aspectratio=169]{beamer}
% \usefonttheme{serif}
%% serif
%% professionalfonts
%% structurebold
%% structureitalicserif
%% structuresmallcapsserif

\mode<presentation> {

\usetheme{Boadilla}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.
\usecolortheme{seahorse}  % The them I usually use, because it resembles the PSI template.

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line
}

\setbeamertemplate{sections/subsections in toc}[sections numbered]  % Controls style of number in the table of contents.

% Packages:
\usepackage{adjustbox}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{animate}
\usepackage{appendixnumberbeamer}
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{cancel}
%\usepackage{dutchcal}
\usepackage{enumitem}
\usepackage{float}
\usepackage{graphicx} % Allows including images
\usepackage{hhline}
\usepackage{listings}
\lstset{
  language=Python,
  basicstyle=\ttfamily,
  mathescape
}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage{multimedia}
\usepackage[authoryear]{natbib}
\usepackage{scrextend}
\changefontsizes{9pt}
\usepackage{siunitx}
\usepackage{soul}
\usepackage{tikz}
\usetikzlibrary{mindmap, trees, arrows, shapes, backgrounds, matrix, decorations.pathreplacing, decorations.pathmorphing, positioning, arrows.meta,plotmarks, calc}
\usepackage{verbatim}
\usepackage{xmpmulti}

% \bibliographystyle{unsrt}  % This style cites with numbers, e.g. [2]
\bibliographystyle{unsrtnat}  % This style cites with names, e.g. [Adelmann, 2019]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Tikz stuff:
\usetikzlibrary{positioning}
\usetikzlibrary{patterns}
\usetikzlibrary{shapes}
\usetikzlibrary{matrix, arrows,decorations.pathmorphing}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{fadings}
\usetikzlibrary{arrows.meta,bending}
\usepackage{tikzscale}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\setbeamertemplate{navigation symbols}{}  % Remove navigation symbols
\setbeamersize{text margin left=1cm,text margin right=1cm}

%% Maths definitions.
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\supp}{supp}
\DeclareMathOperator*{\Var}{\mathrm{Var}}
\DeclareMathOperator*{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\MSE}{\text{MSE}}
\DeclareMathOperator*{\sgn}{\text{sgn}}
\DeclareMathOperator*{\Epsilon}{\mathcal{E}}
\DeclareMathOperator*{\bigO}{\mathcal{O}}
\DeclareMathOperator*{\smalls}{s}
\newcommand{\mvec}[1]{\boldsymbol{#1}}
\newcommand\underrel[2]{\mathrel{\mathop{#2}\limits_{#1}}}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Enter here author information:
\title[]{Discussion DIH} % The short title appears at the bottom of every slide, the full title is only on the title page.

\author{\textbf{Tobia ClaglÃ¼na}}
\institute[LSM, PSI]{AMAS Group\\}
%\date{\today}
\def \myEmail {tobiac@ethz.ch}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\frametitlepsi}[1]{\frametitle{\hspace{0.8cm}\includegraphics[width=1.5cm]{logos/PSI.pdf}\hspace{1.1cm} #1}}

\begin{document}
\setbeamertemplate{caption}{\raggedright\insertcaption\par}
\def\vfilll{\vskip 0pt plus 1filll minus 0pt }
\begin{frame}
  % Title page image:
  \vspace{0.3cm}
  \begin{adjustbox}{width=\paperwidth, center}
    \begin{tikzpicture}
      \centering
      \filldraw[fill=lightgray!40!white, draw = none] (0,0) rectangle (3.0,0.5\textheight);
      % Logos:
      \node[anchor=south west,inner sep=0] at (0.26,3.0)
           {\includegraphics[width=2.5cm]{logos/PSI.pdf}};
      \node[anchor=south west,inner sep=0] at (0.32,1.8)
           {\includegraphics[width=2.4cm]{logos/eth_logo_kurz_pos.eps}};
    \end{tikzpicture}
    \begin{tikzpicture}
      \node (heli) [anchor=south west,inner sep=0] at (0,0)
            {\includegraphics[height=0.5\textheight]{logos/PSI_helicopter}};
    \end{tikzpicture}
    \begin{tikzpicture}
      \filldraw[fill=lightgray!40!white, draw = none] (0,0) rectangle (1,0.5\textheight);
    \end{tikzpicture}
  \end{adjustbox}
  \vspace{0.1cm}\\
  {\usebeamerfont{subtitle} \footnotesize \insertauthor\, ::  \insertinstitute}
  \vspace{0.4cm}
  {\usebeamerfont{title} \LARGE \inserttitle}\\
  \vspace{0.4cm}
  {\usebeamerfont{subtitle} \footnotesize \insertdate}\\
  \vfilll
  \null\hfill\tiny Contact: \url{\myEmail}
\end{frame}

% Table of contents slide, comment this block out to remove it.
%\begin{frame}[noframenumbering]
%\frametitle{Overview}
%\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation.
%\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------
\section{Progress Report}

\begin{frame}
    \frametitle{Mean Statistics}

\begin{figure}[!htb]
\minipage{0.45\textwidth}
  \includegraphics[width=1.1\linewidth]{figures/rmean.pdf}
    \caption{Mean radius in X direction}
  \label{fig:awesome_image2}
\endminipage\hfill
\minipage{0.45\textwidth}%
  \includegraphics[width=1.1\linewidth]{figures/vmeanx.pdf}
    \caption{Mean velocity in X direction}
  \label{fig:awesome_image3}
\endminipage
\end{figure}

\end{frame}

\begin{frame}
    \frametitle{Investigating the Focus Force}

\begin{figure}[!htb]
\minipage{0.40\textwidth}%
  \includegraphics[width=1.1\linewidth]{figures/rrmsx.pdf}
    \caption{Mean velocity in X direction}
  \label{fig:awesome_image3}
\endminipage
\end{figure}

\end{frame}

\begin{frame}
    \frametitle{Correlation Statistics}

\begin{figure}[!htb]
\minipage{0.45\textwidth}
  \includegraphics[width=1.1\linewidth]{figures/rvrmsX_p3m.pdf}
  \label{fig:awesome_image2}
\endminipage\hfill
\minipage{0.45\textwidth}%
  \includegraphics[width=1.1\linewidth]{figures/rvrmsX_langevin.pdf}
  \label{fig:awesome_image3}
\endminipage
\end{figure}

\end{frame}

\begin{frame}
    \frametitle{Comparison to P3M}

     Change in mesh size drastically improves results:

\begin{figure}[!htb]
%\minipage{0.65\textwidth}%
  \includegraphics[width=0.5\linewidth]{figures/comparison_meshsize.pdf}
    \caption{Normalized Emittance of a cold sphere (no collisions), showing diverging emittance.} 
  \label{fig:awesome_image4}
%\endminipage
\end{figure}

\end{frame}

\begin{frame}
    \frametitle{Comparison to P3M}

     \item Constant focusing factor is an important hyperparameter:
     \begin{columns}
         \column{0.50\textwidth}

\begin{figure}[!htb]
%\minipage{0.65\textwidth}%
  \includegraphics[width=0.9\linewidth]{figures/comparison_low_focus.pdf}
    \caption{Decreased Constant Focusing} 
  \label{fig:awesome_image5}
%\endminipage
\end{figure}

         \column{0.50\textwidth}
\begin{figure}[!htb]
%\minipage{0.65\textwidth}%
  \includegraphics[width=0.9\linewidth]{figures/comparison_high_focus.pdf}
    \caption{Increased Constant Focusing} 
  \label{fig:awesome_image6}
%\endminipage
\end{figure}

     \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Conclusion about Results}

\begin{itemize}[label=$\bullet$]
    \item Emittance coincides to some extent with Severin's results
	\item The other statistics are not yet in agreement (though they must have been at some point?)
\end{itemize}

\end{frame}

%\section{Motivation for High-Dimensional UQ: Example from Nuclear Physics}
%\frame[noframenumbering]{\tableofcontents[currentsection]}

%\begin{frame}
  %\frametitlepsi{Definition of Uncertainty Quantification (UQ)}

  %\Large
  %Uncertainty Quantification (UQ) aims to calculate the effect of unknown or uncertain system parameters on the outcome of an experiment or computation.
%\end{frame}
  
%\begin{frame}
  %\frametitlepsi{Definition of Uncertainty Quantification (UQ)}
  
  %Let $f\in L^2(\mathbb{R}^d)$ be a \textbf{computationally expensive} model with 

  %\begin{equation*}
    %f\colon \begin{array}[t]{ >{\displaystyle}r >{{}}c<{{}}  >{\displaystyle}l } 
          %\mathbb{R}^{d} &\to& \mathbb{R} \\ 
          %\mvec x &\mapsto& f(\mvec x)\,.
         %\end{array}
  %\end{equation*}
  %Let $\mvec x = (x_1,x_2,...,x_d)$ be an input with uncertainty $\Delta\mvec x$. \\
  %What is the uncertainty in $f(\mvec x)$?
  
  %\vspace{1cm}

  %\onslide<2->{
    %Common approach, model input as random variable $\mvec X\sim\mathcal{N}(\mvec x, \Sigma)$, with $\Sigma\in\mathbb{R}^{d\times d}$ the covariance matrix (uncertainties and correlations):
    
        %\begin{center}
      %% Flowchart MC UQ
      %\begin{tikzpicture}[node distance = 3.5cm, auto]
        %\tikzstyle{block} = [rectangle, draw, fill=white, 
          %text width=10em, text centered, rounded corners, minimum height=4em]
        
        %% \tikzstyle{line} = [draw, -latex']
        %\tikzstyle{line} = [draw, -{Triangle[scale=2]}]
        
        %\node [block, text width=8em] (1) {
          %$X\sim\mathcal{N}(\vec x,\Sigma)$
          %\includegraphics[width=\textwidth]{plots/posterPlot-SUQ-input}
        %};
        %\node [block, right of=1, text width=8em] (2) {
          %$f(X)$
          %\includegraphics[trim={0 43 0 0}, clip, width=\textwidth]{plots/posterPlot-SUQ-output_v3}
        %};

        %\path [line] (1) -> (2);
      %\end{tikzpicture}
    %\end{center}
  %}

  %\onslide<3->{
    %Concentrate on \textit{Response Variability Methods}: estimate mean and variance of output
    %\[f(\mvec x) = \mu \pm \sigma\,.\]
  %}
  
%\end{frame}

%\begin{frame}
  %\frametitlepsi{Motivation: SNF Characterisation}

  %\onslide<-2>{
    %Nuclear burnup simulations are used to characterise spent nuclear fuel:
    %{
      %\large
      %\begin{equation*}
        %f:\underbrace{
          %\begin{pmatrix}
            %\texttt{Fresh fuel parameters} \\
            %\texttt{Irradiation history} \\
            %\texttt{Reactor parameters}\\
            %\texttt{Nuclear Data}
          %\end{pmatrix}
        %}_{
          %\text{Input with known uncertainty}
        %}
        %\rightarrow
        %\underbrace{
          %\begin{pmatrix}
            %\texttt{Decay Heat} \\
            %\texttt{Isotopic Content}\\
            %\texttt{Criticality}
          %\end{pmatrix}
        %}_{
          %\text{Output with unknown uncertainty}
        %}
      %\end{equation*}
    %}
  %}
  %\onslide<3->{
    %\begin{equation*}
      %\begin{split}
        %f:\mathbb{R}^{15000}&\rightarrow\mathbb{R}\\
        %\text{Nuclear Data}&\rightarrow\text{Decay Heat}
      %\end{split}
    %\end{equation*}
  %}

  %Any uncertainty in outputs will increase the risks and costs of 
  %\begin{figure}
    %\centering
    %\includegraphics[width=0.25\textwidth]{img/transport.PNG}
    %\includegraphics[width=0.25\textwidth]{img/storage.PNG}
    %\includegraphics[width=0.25\textwidth]{img/disposal.PNG}
    %\caption{\large Transport \hspace{1.8cm} Storage \hspace{1.8cm} Disposal}
  %\end{figure}

  %\onslide<2->{
    %\Large
    %\textbf{Accurate estimation of uncertainty saves money and reduces risks.}
  %}

%\end{frame}

%\subsection{Current Methods and Shortcomings}
%\subsubsection{Simple MC}
%\subsubsection{Surrogate Models}
%\frame[noframenumbering]{\tableofcontents[currentsection]}

%\begin{frame}
  %\frametitlepsi{
    %Simple Monte Carlo UQ}

  %\begin{enumerate}[label*=\arabic*.]
  %\item $\quad$
    %\vspace{-1cm}
    %\begin{center}
      %% Flowchart MC UQ
      %\begin{tikzpicture}[node distance = 3.5cm, auto]
        %\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
          %text width=10em, text centered, rounded corners, minimum height=4em]
        
        %% \tikzstyle{line} = [draw, -latex']
        %\tikzstyle{line} = [draw, -{Triangle[scale=2]}]
        
        %\node [block] (1) {
          %Sample inputs $\vec x_1,...,\vec x_N\sim\mathcal{N}(\vec x,\Sigma)$
          %\includegraphics[width=\textwidth]{plots/posterPlot-SUQ-input}
        %};
        %\node [block, right of=1, text width=6em] (2) {
          %Run $f(\vec x_i)$ $N$ times
        %};
        %\node [block, right of=2, text width=12em] (3) {
          %Compute sample mean and variance\\
          %\includegraphics[width=\textwidth]{plots/posterPlot-SUQ-output_v3}
        %};

        %\path [line] (1) -> (2);
        %\path [line] (2) -> (3);
      %\end{tikzpicture}
    %\end{center}

    %\item Compute sample mean and variance
    %\begin{equation*}
      %\mu_N = \frac{1}{N}\sum_{i=1}^N f(\mvec x_i)\,,\quad\sigma^2_N = \frac{1}{N-1}\sum_{i=1}^N \left(f(\mvec x_i) - \sum_{j=1}^N \frac{f(\mvec x_j)}{N}\right)^2\,.
    %\end{equation*}

  %\end{enumerate}
  %\onslide<2->{
    %Simple MC is unbiased, but slow (error$=\sqrt{\MSE}=\bigO\left(\frac{1}{\sqrt{N}}\right)$):
    %\begin{align*}
      %\lim_{N\rightarrow\infty}\mu_N = \E[f]\,,& \quad\text{since }\MSE\Big(\mu_N-\E[f]\Big) = \frac{\Var[f]}{N}\,,\\
      %\lim_{N\rightarrow\infty}\sigma^2_N = \Var[f]\,,& \quad\text{since }\MSE\Big(\sigma^2_N - \Var[f]\Big) = \frac{1}{N}\left(m_4[f] - \frac{N-3}{N-1}{\Var}^2[f]\right)\,.
    %\end{align*}
  %}
%\end{frame}

%\begin{frame}
  %\frametitlepsi{Simple Monte Carlo UQ}

  %Simple MC is the current approach used for nuclear data propagation:
  %\begin{itemize}[label=--]
  %\item MC converges as $\bigO\left(\frac{1}{\sqrt{N}}\right)$, i.e. many simulations required!
  %\item E.g. for SNF characterisation $N\sim 1000$, with each simulation lasting a few hours.
  %\item Expecting $>12000$ fuel assemblies in Switzerland.
  %\item $\Rightarrow$ millions of CPU hours  $\Rightarrow$ \textbf{MC UQ is too slow!}
  %\end{itemize}
%\end{frame}

%\begin{frame}
  %\large
  %In summary: simple MC and surrogate models are inadequate for high-dimensional UQ.
%\end{frame}


%\section{New method: Lasso Monte Carlo}
%\subsection{Multilevel Monte Carlo}
%\subsection{Lasso Regression}
%\frame[noframenumbering]{\tableofcontents[currentsection]}

%\begin{frame}
  %\frametitlepsi{Lasso Monte Carlo}
  %\large
  %Lasso Monte Carlo (LMC) is a new technique that combines two existing methods:
  %\begin{itemize}[label=--]
  %\item Multilevel Monte Carlo (MLMC) \citep{giles_multilevel_2008, krumscheid_quantifying_2020}
  %\item Lasso regression \citep{tibshirani_regression_1996}
  %\end{itemize}

%\end{frame}

%\begin{frame}
  %\frametitlepsi{MLMC}
  %MLMC combines models of different levels of fidelity.

  %\vspace{.5cm}
  %Let $X$ be a random variable, and $f_1,f_2,..., f_L$ be models of increasing accuracy, and increasing computational cost. Then
  %\begin{equation*}
    %\mathbb{E}[f_L(X)] = \mathbb{E}[f_1(X)] + \mathbb{E}[f_2(X) - f_1(X)] + \mathbb{E}[f_3(X) - f_2(X)] + ... + \mathbb{E}[f_{L-1}(X) - f_L(X)]
  %\end{equation*}

  %\onslide<2->{
    %Terms computed with
    %\begin{equation*}
      %\mathbb{E}[f_\ell(X) - f_{\ell-1}(X)] = \frac{1}{N_\ell}\sum_{i=1}^{N_\ell}\{f_\ell(x_i)-f_{\ell-1}(x_i)\},
    %\end{equation*}
    %will converge as $\mathcal{O}\left(\frac{\Var[f_\ell - f_{\ell-1}]}{\sqrt{N_L}}\right)$.
  %}
  
  %\onslide<3->{
    %So if we have
    %\begin{equation*}
      %\Var(f_1) > \Var(f_2-f_1) > \Var(f_3-f_2) > ... > \Var(f_L-f_{L-1}),
    %\end{equation*}
    
    %we require
    %\begin{equation*}
      %N_1 > N_2 > ... > N_L.
    %\end{equation*}
    
    %Overall computational cost is reduced if $N_\ell$ are correctly chosen!
  %}

  %\onslide<4->{
    %Thanks to more recent papers %\cite{bierig_estimation_2016, krumscheid_quantifying_2020}, MLMC can be used for higher order moments.
  %}
%\end{frame}

%\begin{frame}
  %\frametitlepsi{Two-level MC}
  
  %Let
  %\begin{itemize}[label=--]
  %\item $f$ be the true, expensive model, that we evaluate $N$ times: $f(\mvec x_1), f(\mvec x_2),..., f(\mvec x_{N})$.
  %\item $\widetilde f$ a cheap, biased, surrogate model, that we evaluate $N+M$ times, with $M\gg N$:  $\widetilde f(\mvec x_1), \widetilde f(\mvec x_2), ..., \widetilde f(\mvec x_{N})$, and  $\widetilde f(\mvec z_1), \widetilde f(\mvec z_2), ..., \widetilde f(\mvec z_{M})$.
  %\end{itemize}

  %\onslide<2->{
    %Then the estimators are
    %\begin{align*}
      %\mu_{N,M} &= \frac{1}{M}\sum_{i=1}^{M} \widetilde f(\mvec z_i) + \frac{1}{N}\sum_{i=1}^N f(\mvec x_i) - \widetilde f(\mvec x_i) = \widetilde\mu_M + \mu_N - \widetilde\mu_N\,,\\
      %\sigma^2_{N,M} &= \widetilde \sigma^{2}_{M} + \sigma^{2}_{N} - \widetilde \sigma^{2}_N\,.
    %\end{align*}
  %}

  %\onslide<3->{
    %\begin{itemize}[label=--]
    %\item Estimators are unbiased $\lim_{\substack{N\rightarrow\infty\\ M\rightarrow\infty}}\mu_{N,M} = \E[f]\,,\quad\lim_{\substack{N\rightarrow\infty\\ M\rightarrow\infty}}\sigma^2_{N,M} = \Var[f]\,.$
    %\item More accurate than simple MC $\mu_{N},\sigma^2_{N}$, if and only if following conditions are satisfied
      %{\small
        %\begin{align}
            %&\Var[f-\widetilde f] \leq ~\Var[f]\,,\label{eq:cond_a}\\
          %&\resizebox{1.0\hsize}{!}{$m_{2,2}\left[f+\widetilde f, f-\widetilde f\right] + \frac{1}{N-1}\Var[f+\widetilde f]\Var[f-\widetilde f] - \frac{N-2}{N-1}\left(\Var[f]-\Var[\widetilde f]\right)^2 \leq ~ m_4[f] - \frac{N-3}{N-1}{\Var}^2[f]\,.$} \label{eq:cond_b}
        %\end{align}
      %}
    %\end{itemize}
  %}

%\end{frame}

%\begin{frame}
  %\frametitlepsi{Two-level MC}
  %\large
  %Common usage of MLMC:
  %\begin{enumerate}[label*=\arabic*.]
  %\item Gather a training set $\mvec x_1, f(\mvec x_1)$, $\mvec x_2, f(\mvec x_2)$, ..., $\mvec x_{N_{tr}}, f(\mvec x_{N_{tr}})$.
  %\item Train a surrogate model $\widetilde f\sim f$, that is \textbf{fast to evaluate}.
  %\item Evaluate $\widetilde f$ $N+M$ times to obtain $\widetilde f(\mvec x_1), \widetilde f(\mvec x_2), ..., \widetilde f(\mvec x_{N})$, and  $\widetilde f(\mvec z_1), \widetilde f(\mvec z_2), ..., \widetilde f(\mvec z_{M})$.
  %\item Evaluate $f$ $N$ times, to obtain $f(\mvec x_1), f(\mvec x_2),..., f(\mvec x_{N})$.
  %\item Compute 2-level estimators $\mu_{N,M}, \sigma^2_{N,M}$.
  %\end{enumerate}

  %\vspace{1cm}

  %\onslide<2->{
    %\begin{itemize}[label=--]
    %\item Unbiased.
    %\item More accurate than simple MC for a given $N$ (if conditions (\ref{eq:cond_a}, \ref{eq:cond_b})).
    %\item However, bottleneck is still generating the training set $N_{tr}$ (especially in high-dimensional cases).
    %\end{itemize}
  %}
  
%\end{frame}

%\begin{frame}
  %\frametitlepsi{Choosing Surrogate Model}
  %\large
  %How to choose a surrogate model that satisfies convergence conditions, and can be trained with a small training set $N_{tr}\ll d$?

  %\vspace{.5cm}
  %\onslide<2->{
    %Lasso regression technique fits a sparse linear model:
    %\begin{equation*}
      %\widetilde f(\mvec x) = \mvec\beta\cdot\mvec x,\quad \text{with }\mvec\beta\text{ sparse,}
    %\end{equation*}

    %by minimising loss function
    %\begin{equation*}
      %\mathcal{L}(\mvec\beta) = \underbrace{\frac{1}{2}\sum_{i=1}^{N_{tr}}\left(f(\mvec x_i) - \mvec\beta\cdot\mvec x_i \right)^2}_{\text{OLS loss}} + \underbrace{\lambda||\mvec{\beta}||_1}_{\text{Regularisation term}}\,,
    %\end{equation*}

    %with $\lambda>0$ a chosen regularisation constant.
  %}

  %\onslide<3->{
    %\begin{itemize}[label=--]
    %\item Lasso can be trained for small training sets, without overfitting.
    %\item Does it satisfy the convergence conditions (\ref{eq:cond_a}, \ref{eq:cond_b})?
    %\end{itemize}

    %}

  
%\end{frame}

%\begin{frame}
  %\frametitlepsi{Choosing Surrogate Model}
  %\large
  
  %Does Lasso $\widetilde f$ satisfy the convergence conditions (\ref{eq:cond_a}, \ref{eq:cond_b})?

    %%%   \item Lasso satisfies convergence conditions (under some conditions on $f$), i.e. if $\widetilde f$ is a Lasso model, $\mu_{N,M},\sigma_{N,M}$ will be equally or more accurate than simple MC $\mu_N,\sigma_N$.

  %Condition \eqref{eq:cond_b}, is unfortunately not guaranteed.

  %\vspace{1cm}
  %\onslide<2->{
    %However, if $f$ is a \textit{noisy linear function}
    %\[f(\mvec x) = \mvec \alpha\cdot\mvec x + \Epsilon\,,\quad\text{with} \Epsilon\sim\mathcal{N}(0,\varepsilon)\]

    %then condition \eqref{eq:cond_b} is guaranteed! This is true to first order for any $f$:
    %\[f(\mvec x + \delta\mvec x) = f(\mvec x_0) + \delta\mvec x\cdot\mvec\nabla f(\mvec x_0) + \mathcal{O}\left(||\delta\mvec x||^2\right).\]
  %}

  %\onslide<3->{
    %I.e. the two-level estimator $\sigma^2_{N,M}$ with Lasso, will often converge faster than simple MC, and is guaranteed to do so under certain conditions on $f$.
  %}
  
%\end{frame}

%\begin{frame}
  %\frametitlepsi{Lasso Monte Carlo}
  %\large

  %Two-level MC + Lasso:
  %\begin{enumerate}[label*=\arabic*.]
  %\item Gather \tikz[remember picture,inner sep=0pt] \node (top)  {small set}; $\mvec x_1, f(\mvec x_1)$, $\mvec x_2, f(\mvec x_2)$, ..., $\mvec x_{N_{tr}}, f(\mvec x_{N_{tr}})$. 
  %\item Train a Lasso model $\widetilde f\sim f$.
  %\item Evaluate $\widetilde f$ $N+M$ times, with $M\gg N$.
  %\item Evaluate $f$ $N$ times.\tikz[remember picture,inner sep=0pt] \node (bot)  {};
  %\item Compute 2-level estimators $\mu_{N,M}, \sigma^2_{N,M}$
  %\end{enumerate}

  %\onslide<2->{
    %\begin{tikzpicture}[remember picture,overlay]
      %\node[red, draw,align=left] at (9.0,2.6) {Reuse the same set for training!};
      %\draw[arrows=->,red,line width=2pt,opacity=0.90] (bot) ++ (-0.25em,1ex) .. controls ++ (5,0) and ++(0,2) .. ($(top)+(4pt,1.35ex)$);
    %\end{tikzpicture}
  %}

%\end{frame}

%\begin{frame}
  %\frametitlepsi{Lasso Monte Carlo}
  %\large

  %LMC algorithm:
  %\begin{enumerate}[label*=\arabic*.]
  %\item Evaluate $f$ $N$ times: $\mvec x_1, f(\mvec x_1)$, $\mvec x_2, f(\mvec x_2)$, ..., $\mvec x_{N}, f(\mvec x_{N})$. 
  %\item Train a Lasso model $\widetilde f\sim f$.
  %\item Evaluate $\widetilde f$ $N+M$ times, with $M\gg N$.
  %\item Compute 2-level estimators $\mu_{N,M}, \sigma^2_{N,M}$
  %\end{enumerate}

  %\vspace{1cm}

  %\onslide<2->{
    %\begin{itemize}[label=--]
    %\item Unbiased.
    %\item Faster (or equal) convergence than simple MC for a given $N$.
    %\item Surrogate model trained \textit{for free} (no extra simulations required).
    %\item Note: this version of LMC omits some steps (splitting and averaging), see full algorithm in paper.
    %\end{itemize}
  %}
  
%\end{frame}

%\begin{frame}[fragile]
  %\frametitlepsi{Lasso Monte Carlo}

  %An actual code example:
  
  %\begin{lstlisting}
%>>> from sklearn.linear_model import LassoCV, Lasso
%>>> from LMC.classLMC import LassoMC

%>>> lmc = LassoMC(regressor = Lasso(lambda = 0.02),
                 %random_state = seed, verbose = True,
                 %validation_method = '5Fold')
%>>> N = 150; M = 6000
%>>> Xs = get_inputs(N)
%>>> ys = [my_simulation(x) for x in Xs]
%>>> Zs = get_inputs(M)
%>>> lmc.get_single_estimate(Xtrain = Xs,
                           %ytrain = ys,
                           %Xtest = Zs)
%Ntr = 150 labelled samples, Ntest = 6000 unlabelled samples
%MC estimates: 5234.470666666667 +- 174.65316984757996
%LMC estimates: 5246.745253371253 +- 192.6719429998857
  %\end{lstlisting}

  %\onslide<2->{
    %\begin{tikzpicture}[remember picture,overlay]
      %\draw[red, very thick] (4.6,3.0) circle [x radius=3.1cm, y radius=.28cm];
      %\node[red, draw,align=left] at (9.0,3.0) {Most expensive\\ step of the algorithm};
    %\end{tikzpicture}
  %}
%\end{frame}


%\section{Benchmarks}
%\frame[noframenumbering]{\tableofcontents[currentsection]}

%\begin{frame}
  %\frametitlepsi{SNF Characterisation}

  %\begin{eqnarray*}
    %f\colon  \mathbb{R}^{15\,557} &\to& \mathbb{R}  \\
    %\text{Nuclear Data} &\mapsto& \text{Decay Heat} 
  %\end{eqnarray*}
  
  %\only<1>{
    %Plots show increasing $N$, and fixed $M=6000$.
    %\begin{figure}
      %\centering
      %\includegraphics[width=\textwidth]{plots/lassoMC-DH-conv_500days-model=LassoCV_method=5Fold_withNmax1000_manylines.pdf}
    %\end{figure}
  %}
  
  %\only<2>{
    %\begin{figure}
      %\centering
      %\includegraphics[width=\textwidth]{plots/lassoMC-DH-conv_500days-model=LassoCV_method=5Fold_withNmax1000.pdf}
    %\end{figure}
  %}
  %\onslide<3->{
    %\begin{figure}
      %\centering
      %\includegraphics[width=\textwidth]{plots/lassoMC-DH-errorconv_500days-model=LassoCV_method=5Fold_withNmax1000.pdf}
    %\end{figure}

    %To obtain a $1\%$ error in estimations, simple MC requires $N=1000$ expensive simulations $f$, while LMC requires $N=200$. I.e. \textbf{5 times speedup thanks to LMC}.
  %}

  
%\end{frame}

%\begin{frame}
  %\frametitlepsi{SNF Characterisation}
  %\large

  %Fixed $N=150$ and $M=6000$.
  %\begin{eqnarray*}
    %f\colon  \mathbb{R}^{15\,557} &\to& \mathbb{R}  \\
    %\text{Nuclear Data} &\mapsto& \text{Isotopic Content} 
  %\end{eqnarray*}

  
  %Predicting different quantities gives different improvements. But \textbf{LMC is always equal or better than simple MC}.

    %\begin{figure}
      %\centering
      %\includegraphics[trim={0 0 0 30}, clip, width=.75\textwidth]{plots/IC_methodComparison_allYears_500days_onlyStd.pdf}
    %\end{figure}

  
%\end{frame}



%\section{Conclusion}
%\frame[noframenumbering]{\tableofcontents[currentsection]}

%\begin{frame}
  %\frametitlepsi{Conclusions and Further Work}

  %\begin{itemize}[label=--]
  %\item LMC converges up to \textbf{5 times faster than simple MC}! I.e. same results with $20\%$ of the computing resources.
  %\item LMC is often advantageous over simple MC and surrogate models in high-dimensional settings.
  %\item Given a set of simulations  $\mvec x_1, f(\mvec x_1)$, $\mvec x_2, f(\mvec x_2)$, ..., $\mvec x_{N}, f(\mvec x_{N})$, the LMC estimates can be obtained without any extra simulations.
  %\item Could in principle be used with any surrogate model, as long as it is regularised
    %\vspace{1cm}
  %\item The speedup is not constant, it's very dependent on $f$.
  %\item Unfortunately, theoretical guarantee of faster convergence is conditioned on:
    %\begin{itemize}[label=--]
    %\item $f$ being close to \textit{a noisy linear function}.
    %\item an optimal choice of regularisation parameter $\lambda$ (chosen empirically so far).
    %\end{itemize}
  %\end{itemize}

%\end{frame}

%% Farewell and bibliography
%\begin{frame}
  %\frametitle{}
  %\begin{adjustbox}{width=\paperwidth, center}
  %\begin{tikzpicture}
    %\node (heli) [anchor=south west,inner sep=0] at (0,0)
          %{\includegraphics[width=.9\textwidth]{logos/PSI_helicopter}};
          %\node [anchor=south west, inner sep = 4,fill=white, opacity=0.8] at (0.0, 2.0)
                %{
                  %\parbox{3.8cm}{\centering\color{black}
                    %Thank you for your attention.\\
                    
                    %\vspace{1cm}
                           %{\large Questions?}
                           %\vspace{1cm}
                  %}
                  
                %};
  %\end{tikzpicture}
  %\end{adjustbox}
%\end{frame}

%\begin{frame}
  %\frametitle{References}
             %{\scriptsize
               %\begin{multicols}{2}
                 %\bibliography{references.bib}
               %\end{multicols}
             %}
%\end{frame}

%% Here backup slides start.
%\appendix
%\frame{\Large Extra Slides}
%\begin{frame}
  %\frametitlepsi{Results}
    %\Large
    %Backup slides blabla. These slides don't appear on page numbering.

    
    %\includegraphics[width=0.4\textwidth]{plots/posterPlot-SUQ-output.png}
%\end{frame}

%\begin{frame}
  %\frametitlepsi{Results}
    %\Large
    %Backup slides blabla
%\end{frame}

\end{document} 
